\BOOKMARK [1][-]{section.1}{Probability Calculus}{}% 1
\BOOKMARK [1][-]{section.2}{Linear Conditioning \(e.g y=Ax+b+N\(0,y\)\)}{}% 2
\BOOKMARK [1][-]{section.3}{Bayesian Learning \(Get posterior of \040and predict\)}{}% 3
\BOOKMARK [2][-]{subsection.3.1}{Bayesian Regression}{section.3}% 4
\BOOKMARK [3][-]{subsubsection.3.1.1}{MAP \040RR}{subsection.3.1}% 5
\BOOKMARK [3][-]{subsubsection.3.1.2}{Posterior p\(w X, y\)=N\(w ; , \)}{subsection.3.1}% 6
\BOOKMARK [3][-]{subsubsection.3.1.3}{Predictions p\(y* X, y, x*\)}{subsection.3.1}% 7
\BOOKMARK [3][-]{subsubsection.3.1.4}{Recursive Bayesian Updates}{subsection.3.1}% 8
\BOOKMARK [2][-]{subsection.3.2}{Kalman Filters}{section.3}% 9
\BOOKMARK [3][-]{subsubsection.3.2.1}{Parameter Estimation yt = x + t, tN\(0,y2\)}{subsection.3.2}% 10
\BOOKMARK [3][-]{subsubsection.3.2.2}{General Kalman Update \(Gaussian\)}{subsection.3.2}% 11
\BOOKMARK [2][-]{subsection.3.3}{Kernel - Lin. method \(BLR\) on nonlin. transf. data}{section.3}% 12
\BOOKMARK [2][-]{subsection.3.4}{GP}{section.3}% 13
\BOOKMARK [3][-]{subsubsection.3.4.1}{Model Selection, \040 \040= argmaxp\(y X, \)}{subsection.3.4}% 14
\BOOKMARK [3][-]{subsubsection.3.4.2}{Fast GP Methods}{subsection.3.4}% 15
\BOOKMARK [1][-]{section.4}{Bayesian learning}{}% 16
\BOOKMARK [2][-]{subsection.4.1}{Approximate Inference}{section.4}% 17
\BOOKMARK [2][-]{subsection.4.2}{Variational Inference q* argminq Q K L\(q \040p\)}{section.4}% 18
\BOOKMARK [2][-]{subsection.4.3}{KL-Divergence \(non-negative\)}{section.4}% 19
\BOOKMARK [3][-]{subsubsection.4.3.1}{Inference as Optimization \(ELBO\)}{subsection.4.3}% 20
\BOOKMARK [3][-]{subsubsection.4.3.2}{Hoeffding's Inequality}{subsection.4.3}% 21
\BOOKMARK [1][-]{section.5}{Markov-Chain Monte Carlo \(MCMC\)}{}% 22
\BOOKMARK [2][-]{subsection.5.1}{Metropolis Hastings \(MH\)}{section.5}% 23
\BOOKMARK [2][-]{subsection.5.2}{Gibbs Sampling \(Random Order, Practical Variant\)}{section.5}% 24
\BOOKMARK [2][-]{subsection.5.3}{MCMC for Continuous RVs: p\(x\)=1Z exp\(-f\(x\)\)}{section.5}% 25
\BOOKMARK [1][-]{section.6}{Bayesian Neural Networks}{}% 26
\BOOKMARK [1][-]{section.7}{Bayesian Learning \(uncertainty decides data\)}{}% 27
\BOOKMARK [2][-]{subsection.7.1}{Optimizing Mutual Information}{section.7}% 28
\BOOKMARK [2][-]{subsection.7.2}{Active learning for classification}{section.7}% 29
\BOOKMARK [2][-]{subsection.7.3}{Bayesian Optimization}{section.7}% 30
\BOOKMARK [3][-]{subsubsection.7.3.1}{Optimistic Bayesian Optimization with GPs}{subsection.7.3}% 31
\BOOKMARK [1][-]{section.8}{Markov Decision Processes\(MDP\)}{}% 32
\BOOKMARK [2][-]{subsection.8.1}{Policy Iteration: , V\(x\) G}{section.8}% 33
\BOOKMARK [2][-]{subsection.8.2}{Value Iteration V0\(x\) = maxa r\(x, a\)}{section.8}% 34
\BOOKMARK [2][-]{subsection.8.3}{POMDP = Belief-state MDP}{section.8}% 35
\BOOKMARK [1][-]{section.9}{Reinforcement Learning \(RL\)}{}% 36
\BOOKMARK [2][-]{subsection.9.1}{Model-based RL}{section.9}% 37
\BOOKMARK [3][-]{subsubsection.9.1.1}{t Greedy}{subsection.9.1}% 38
\BOOKMARK [3][-]{subsubsection.9.1.2}{Rmax Algorithm}{subsection.9.1}% 39
\BOOKMARK [3][-]{subsubsection.9.1.3}{Receding-Horizon/Model-Predictive control \(MPC\)}{subsection.9.1}% 40
\BOOKMARK [3][-]{subsubsection.9.1.4}{MPC for stochastic transition models}{subsection.9.1}% 41
\BOOKMARK [3][-]{subsubsection.9.1.5}{Unknown Dynamics \(f and r are unknown\)}{subsection.9.1}% 42
\BOOKMARK [2][-]{subsection.9.2}{Model-free RL}{section.9}% 43
\BOOKMARK [3][-]{subsubsection.9.2.1}{Temporal Difference \(TD\)-Learning}{subsection.9.2}% 44
\BOOKMARK [3][-]{subsubsection.9.2.2}{Q-Learning}{subsection.9.2}% 45
\BOOKMARK [3][-]{subsubsection.9.2.3}{Policy Gradients Methods}{subsection.9.2}% 46
\BOOKMARK [1][-]{section.10}{REINFORCE Algorithm}{}% 47
\BOOKMARK [2][-]{subsubsection.10.0.1}{Actor-Critic \(AC\) Algorithm}{section.10}% 48
\BOOKMARK [3][-]{subsubsection.10.0.2}{A2C Algorithm: Variance reduction via baselines}{subsubsection.10.0.1}% 49
\BOOKMARK [3][-]{subsubsection.10.0.3}{Replace exact maximum by parametrized policy}{subsubsection.10.0.1}% 50
\BOOKMARK [3][-]{subsubsection.10.0.4}{Deep Deterministic Policy Gradients \(DDPG\)}{subsubsection.10.0.1}% 51
\BOOKMARK [1][-]{section.11}{Reinforcement Learning via Function Approximation}{}% 52
\BOOKMARK [2][-]{subsection.11.1}{Parametric Value Function Approximation}{section.11}% 53
\BOOKMARK [3][-]{subsubsection.11.1.1}{Examples}{subsection.11.1}% 54
\BOOKMARK [2][-]{subsection.11.2}{Policy Search Methods \(Deal. w/ large action sets\)}{section.11}% 55
\BOOKMARK [1][-]{section.12}{Langevin}{}% 56
